{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation using Augmentation\n",
    "For this class we will conduct model validation using augmentation, we will especially use the package [Augmenty](https://kennethenevoldsen.github.io/augmenty/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We will need to set up a few things before we start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages:\n",
    "For this tutorial you will need the following packages:\n",
    "\n",
    "- spaCy and augmenty are used for the augmentation\n",
    "- transformers are use to run the model we wish to validate\n",
    "- danlp is used to download the dataset we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting augmenty\n",
      "  Using cached augmenty-0.0.9-py3-none-any.whl (61 kB)\n",
      "Collecting spacy==3.1.1\n",
      "  Using cached spacy-3.1.1-cp36-cp36m-win_amd64.whl (11.8 MB)\n",
      "Collecting transformers==4.2.2\n",
      "  Using cached transformers-4.2.2-py3-none-any.whl (1.8 MB)\n",
      "Collecting danlp==0.0.12\n",
      "  Using cached danlp-0.0.12-py3-none-any.whl (71 kB)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (0.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (59.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (4.62.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (3.0.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (8.0.10)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (1.19.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (2.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (3.7.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (2.0.6)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Using cached typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (21.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (0.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (3.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy==3.1.1) (2.4.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from transformers==4.2.2) (0.0.46)\n",
      "Requirement already satisfied: filelock in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from transformers==4.2.2) (3.1.0)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from transformers==4.2.2) (0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from transformers==4.2.2) (2021.11.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from transformers==4.2.2) (4.8.1)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Using cached tokenizers-0.9.4-cp36-cp36m-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: pandas in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from danlp==0.0.12) (1.1.5)\n",
      "Collecting tweepy\n",
      "  Using cached tweepy-4.4.0-py2.py3-none-any.whl (65 kB)\n",
      "Collecting pyconll\n",
      "  Using cached pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
      "Collecting conllu\n",
      "  Using cached conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from catalogue<2.1.0,>=2.0.4->spacy==3.1.1) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from packaging>=20.0->spacy==3.1.1) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from pathy>=0.3.5->spacy==3.1.1) (5.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.1) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.1) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.1) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.1) (3.2)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from thinc<8.1.0,>=8.0.8->spacy==3.1.1) (2.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.1.1) (0.4.4)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from jinja2->spacy==3.1.1) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from pandas->danlp==0.0.12) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from pandas->danlp==0.0.12) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from sacremoses->transformers==4.2.2) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from sacremoses->transformers==4.2.2) (1.0.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.0.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from tweepy->danlp==0.0.12) (1.3.0)\n",
      "Requirement already satisfied: immutables>=0.9 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.8->spacy==3.1.1) (0.16)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from requests-oauthlib<2,>=1.0.0->tweepy->danlp==0.0.12) (3.1.1)\n",
      "Installing collected packages: click, typer, tweepy, tokenizers, spacy, pyconll, conllu, transformers, danlp, augmenty\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.1\n",
      "    Uninstalling click-8.0.1:\n",
      "      Successfully uninstalled click-8.0.1\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.4.0\n",
      "    Uninstalling typer-0.4.0:\n",
      "      Successfully uninstalled typer-0.4.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.1.3\n",
      "    Uninstalling spacy-3.1.3:\n",
      "      Successfully uninstalled spacy-3.1.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.12.3\n",
      "    Uninstalling transformers-4.12.3:\n",
      "      Successfully uninstalled transformers-4.12.3\n",
      "Successfully installed augmenty-0.0.9 click-7.1.2 conllu-4.4.1 danlp-0.0.12 pyconll-3.1.0 spacy-3.1.1 tokenizers-0.9.4 transformers-4.2.2 tweepy-4.4.0 typer-0.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\louis\\OneDrive - Aarhus universitet\\Masters\\1. Semester\\Natural Language Processing\\nlp_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting da-core-news-lg==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/da_core_news_lg-3.1.0/da_core_news_lg-3.1.0-py3-none-any.whl (573.2 MB)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from da-core-news-lg==3.1.0) (3.1.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (21.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (3.0.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (2.26.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (4.62.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (1.19.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (59.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from thinc<8.1.0,>=8.0.8->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (2.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (0.4.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (2.0.1)\n",
      "Requirement already satisfied: immutables>=0.9 in c:\\users\\louis\\onedrive - aarhus universitet\\masters\\1. semester\\natural language processing\\nlp_env\\lib\\site-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.8->spacy<3.2.0,>=3.1.0->da-core-news-lg==3.1.0) (0.16)\n",
      "Installing collected packages: da-core-news-lg\n",
      "Successfully installed da-core-news-lg-3.1.0\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('da_core_news_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 13:52:34.833208: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-12-02 13:52:34.834955: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\louis\\OneDrive - Aarhus universitet\\Masters\\1. Semester\\Natural Language Processing\\nlp_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install augmenty spacy==3.1.1 transformers==4.2.2 danlp==0.0.12\n",
    "!python -m spacy download da_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "For this dataset we will be using [DKHate](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dkhate). The DKHate dataset contains user-generated comments from social media platforms (Facebook and Reddit) annotated for various types and target of offensive language. Note that only labels for the sub-task A (Offensive language identification), i.e. NOT (Not Offensive) / OFF (Offensive), are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.datasets import DKHate\n",
    "import pandas as pd\n",
    "dkhate = DKHate()\n",
    "test, train = dkhate.load_with_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make everything run faster we will only be using a subsample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 20\n",
    "\n",
    "# make sure to sample evenly from the two samples\n",
    "n_labels = len(test[\"subtask_a\"].unique())\n",
    "samples_pr_lab = samples//n_labels\n",
    "\n",
    "off = test[test[\"subtask_a\"] == \"OFF\"].sample(samples_pr_lab)\n",
    "not_off = test[test[\"subtask_a\"] == \"NOT\"].sample(samples_pr_lab)\n",
    "mini_test = pd.concat([off, not_off])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the data using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Jeg ville ønske jeg kunne anerkende [KORRUPT P...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Fuck perkerne. ses på 4chan, tabere.</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>Fuckkkkk det her er mig....</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hvordan i helvede fik de overhovedet dit numme...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>@USER ryger du hash. ???</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>Var vi de eneste røvhuller som dyppede karamel...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Normalt synes jeg Marx var lige højreorientere...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545</th>\n",
       "      <td>Potentiale til månedens billede, lige der.  De...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>Det fandme en fugtig migmig det der</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>uuh, denne her bliver nok upopulær, men jeg er...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Jeg mener ikke at en mand er mere troværdig. J...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Tillykke til både Danmark og Island  - Hilsen ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>Selv mod Trump kæmper guderne forgæves han er ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>Håber der er Fodbold på den tid i TV</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Hun er jo ikke dum. Jeg respektere da helt kla...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>Vi er til grin! Totalt til grin. Stik ham en m...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>Det er øverst fra Kronborg hvis nogen skulle v...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Jamen går rødekors penge ikke til dem i forvejen</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Direktøren på Huffington Post er en dansker!</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>Har han kommenteret i dag?</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet subtask_a\n",
       "id                                                               \n",
       "924   Jeg ville ønske jeg kunne anerkende [KORRUPT P...       OFF\n",
       "159                Fuck perkerne. ses på 4chan, tabere.       OFF\n",
       "658                         Fuckkkkk det her er mig....       OFF\n",
       "9     Hvordan i helvede fik de overhovedet dit numme...       OFF\n",
       "137                            @USER ryger du hash. ???       OFF\n",
       "1139  Var vi de eneste røvhuller som dyppede karamel...       OFF\n",
       "544   Normalt synes jeg Marx var lige højreorientere...       OFF\n",
       "3545  Potentiale til månedens billede, lige der.  De...       OFF\n",
       "3326                Det fandme en fugtig migmig det der       OFF\n",
       "753   uuh, denne her bliver nok upopulær, men jeg er...       OFF\n",
       "777   Jeg mener ikke at en mand er mere troværdig. J...       NOT\n",
       "621   Tillykke til både Danmark og Island  - Hilsen ...       NOT\n",
       "1089  Selv mod Trump kæmper guderne forgæves han er ...       NOT\n",
       "927                Håber der er Fodbold på den tid i TV       NOT\n",
       "379   Hun er jo ikke dum. Jeg respektere da helt kla...       NOT\n",
       "1487  Vi er til grin! Totalt til grin. Stik ham en m...       NOT\n",
       "2532  Det er øverst fra Kronborg hvis nogen skulle v...       NOT\n",
       "108    Jamen går rødekors penge ikke til dem i forvejen       NOT\n",
       "860        Direktøren på Huffington Post er en dansker!       NOT\n",
       "3122                         Har han kommenteret i dag?       NOT"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "For this dataset we will be using a model trained on the train set of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 905/905 [00:00<00:00, 908kB/s]\n",
      "Downloading: 100%|██████████| 443M/443M [00:35<00:00, 12.5MB/s]\n",
      "Downloading: 100%|██████████| 253k/253k [00:00<00:00, 563kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 37.4kB/s]\n",
      "Downloading: 100%|██████████| 342/342 [00:00<00:00, 344kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"DaNLP/da-bert-hatespeech-detection\"\n",
    "pipe = pipeline(\"sentiment-analysis\", # text classification == sentiment analysis (don't ask me why, but they removed textcat in the latest version)\n",
    "               model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly check the output using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'offensive', 'score': 0.9902198910713196},\n",
       " {'label': 'not offensive', 'score': 0.9998297691345215}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe([\"Gamle stupide idiot\", \"Lækkert vejr i dag\"]) # old stupid idiot, nice weather today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly apply this model to all our examples and save them in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = mini_test[\"tweet\"].to_list()\n",
    "\n",
    "def apply(texts):\n",
    "    output = pipe(texts, truncation=True) #truncate if it is too long for the model to handle it\n",
    "    return [t[\"score\"] if t[\"label\"] == \"offensive\" else 1 - t[\"score\"] for t in output]\n",
    "\n",
    "\n",
    "# first without augmentations\n",
    "mini_test[\"p_offensive_no_aug\"] = apply(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>p_offensive_no_aug</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Jeg ville ønske jeg kunne anerkende [KORRUPT P...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.993760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Fuck perkerne. ses på 4chan, tabere.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.995237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>Fuckkkkk det her er mig....</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.013337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hvordan i helvede fik de overhovedet dit numme...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.972994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>@USER ryger du hash. ???</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.997740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>Var vi de eneste røvhuller som dyppede karamel...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.926999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Normalt synes jeg Marx var lige højreorientere...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.125608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545</th>\n",
       "      <td>Potentiale til månedens billede, lige der.  De...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.001631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>Det fandme en fugtig migmig det der</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.022390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>uuh, denne her bliver nok upopulær, men jeg er...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.021918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Jeg mener ikke at en mand er mere troværdig. J...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Tillykke til både Danmark og Island  - Hilsen ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>Selv mod Trump kæmper guderne forgæves han er ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.003527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>Håber der er Fodbold på den tid i TV</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Hun er jo ikke dum. Jeg respektere da helt kla...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.006584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>Vi er til grin! Totalt til grin. Stik ham en m...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.040093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>Det er øverst fra Kronborg hvis nogen skulle v...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.001530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Jamen går rødekors penge ikke til dem i forvejen</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Direktøren på Huffington Post er en dansker!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.004052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>Har han kommenteret i dag?</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet subtask_a  \\\n",
       "id                                                                  \n",
       "924   Jeg ville ønske jeg kunne anerkende [KORRUPT P...       OFF   \n",
       "159                Fuck perkerne. ses på 4chan, tabere.       OFF   \n",
       "658                         Fuckkkkk det her er mig....       OFF   \n",
       "9     Hvordan i helvede fik de overhovedet dit numme...       OFF   \n",
       "137                            @USER ryger du hash. ???       OFF   \n",
       "1139  Var vi de eneste røvhuller som dyppede karamel...       OFF   \n",
       "544   Normalt synes jeg Marx var lige højreorientere...       OFF   \n",
       "3545  Potentiale til månedens billede, lige der.  De...       OFF   \n",
       "3326                Det fandme en fugtig migmig det der       OFF   \n",
       "753   uuh, denne her bliver nok upopulær, men jeg er...       OFF   \n",
       "777   Jeg mener ikke at en mand er mere troværdig. J...       NOT   \n",
       "621   Tillykke til både Danmark og Island  - Hilsen ...       NOT   \n",
       "1089  Selv mod Trump kæmper guderne forgæves han er ...       NOT   \n",
       "927                Håber der er Fodbold på den tid i TV       NOT   \n",
       "379   Hun er jo ikke dum. Jeg respektere da helt kla...       NOT   \n",
       "1487  Vi er til grin! Totalt til grin. Stik ham en m...       NOT   \n",
       "2532  Det er øverst fra Kronborg hvis nogen skulle v...       NOT   \n",
       "108    Jamen går rødekors penge ikke til dem i forvejen       NOT   \n",
       "860        Direktøren på Huffington Post er en dansker!       NOT   \n",
       "3122                         Har han kommenteret i dag?       NOT   \n",
       "\n",
       "      p_offensive_no_aug  \n",
       "id                        \n",
       "924             0.993760  \n",
       "159             0.995237  \n",
       "658             0.013337  \n",
       "9               0.972994  \n",
       "137             0.997740  \n",
       "1139            0.926999  \n",
       "544             0.125608  \n",
       "3545            0.001631  \n",
       "3326            0.022390  \n",
       "753             0.021918  \n",
       "777             0.000148  \n",
       "621             0.000064  \n",
       "1089            0.003527  \n",
       "927             0.000501  \n",
       "379             0.006584  \n",
       "1487            0.040093  \n",
       "2532            0.001530  \n",
       "108             0.000141  \n",
       "860             0.004052  \n",
       "3122            0.000324  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_test #inspecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioural check using Augmentation\n",
    "\n",
    "In the following we want to examine the behavioural consistency of the model using augmentation. The idea is to check the behavioural consistently of the model for instance if we introduce slight spelling errors we the model should still be able to recognize names. If this is not the case it might be unwise to apply the model to domains where spelling errors are common such as social media.  \n",
    "\n",
    "![](img/aug.png)\n",
    "**Figure 1**: Examples of augmentation applied by Enevoldsen et al. (2020) and what domains they might be of relevance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenty\n",
    "For the augmentation we will be using the package augmenty, the following provides a brief introduction to it.\n",
    "\n",
    "**NOTE**: You are naturally not forced to use augmenty, you implement your own augmenters i.e. the following example with uppercasing is easy to implement by hand.  For example if you want to examine the effect of questionmarks you could make the augmentation:\n",
    "```py\n",
    "q_aug = [text + \"?\" for text in texts]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy.orth_variants.v1\n",
      "spacy.lower_case.v1\n",
      "random_casing.v1\n",
      "char_replace_random.v1\n",
      "char_replace.v1\n",
      "keystroke_error.v1\n",
      "remove_spacing.v1\n",
      "char_swap.v1\n",
      "random_starting_case.v1\n",
      "conditional_token_casing.v1\n",
      "token_dict_replace.v1\n",
      "wordnet_synonym.v1\n",
      "token_replace.v1\n",
      "word_embedding.v1\n",
      "grundtvigian_spacing_augmenter.v1\n",
      "spacing_insertion.v1\n",
      "token_swap.v1\n",
      "token_insert.v1\n",
      "token_insert_random.v1\n",
      "duplicate_token.v1\n",
      "random_synonym_insertion.v1\n",
      "ents_replace.v1\n",
      "per_replace.v1\n",
      "ents_format.v1\n",
      "upper_case.v1\n",
      "spongebob.v1\n",
      "da_æøå_replace.v1\n",
      "da_historical_noun_casing.v1\n"
     ]
    }
   ],
   "source": [
    "import augmenty\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"da_core_news_lg\")\n",
    "\n",
    "# a list of augmenters from the augmenty module\n",
    "for augmenter in augmenty.augmenters():\n",
    "    print(augmenter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list naturally does not give you all the information you need. You can always examine a specific augmenter more en detain in the [documentation](https://kennethenevoldsen.github.io/augmenty/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try one of the augmenters. We can use the `augmenty.load` as a common interface for all augmenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an augmenter\n",
    "upper_case_augmenter = augmenty.load(\"upper_case.v1\", level=1.00) # augment 100% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These augmenters are made to work on the SpaCy data class Examples which allows for much more detailed augmentation, however augmenty have utility function to allow us to use them for strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THIS IS AN EXAMPLE', 'AND ANOTHER ONE']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\"this is an example\", \"and another one\"]\n",
    "aug_texts = augmenty.texts(examples, augmenter=upper_case_augmenter, nlp=nlp)\n",
    "list(aug_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is uppercasing more offensive?\n",
    "\n",
    "Now we will can apply our model to the augmented examples to see if it changes predictions of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_texts = augmenty.texts(texts, augmenter=upper_case_augmenter, nlp=nlp)\n",
    "mini_test[\"p_offensive_upper\"] = apply(list(aug_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the output of our models we quickly see that it doesn't change the result at all! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>p_offensive_no_aug</th>\n",
       "      <th>p_offensive_upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Jeg ville ønske jeg kunne anerkende [KORRUPT P...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.993760</td>\n",
       "      <td>0.993760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Fuck perkerne. ses på 4chan, tabere.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.995237</td>\n",
       "      <td>0.995237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>Fuckkkkk det her er mig....</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.013337</td>\n",
       "      <td>0.013337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hvordan i helvede fik de overhovedet dit numme...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.972994</td>\n",
       "      <td>0.972994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>@USER ryger du hash. ???</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.997740</td>\n",
       "      <td>0.997740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>Var vi de eneste røvhuller som dyppede karamel...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.926999</td>\n",
       "      <td>0.926999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Normalt synes jeg Marx var lige højreorientere...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.125608</td>\n",
       "      <td>0.125608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545</th>\n",
       "      <td>Potentiale til månedens billede, lige der.  De...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.001631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>Det fandme en fugtig migmig det der</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.022390</td>\n",
       "      <td>0.022390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>uuh, denne her bliver nok upopulær, men jeg er...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.021918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Jeg mener ikke at en mand er mere troværdig. J...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Tillykke til både Danmark og Island  - Hilsen ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>Selv mod Trump kæmper guderne forgæves han er ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.003527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>Håber der er Fodbold på den tid i TV</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Hun er jo ikke dum. Jeg respektere da helt kla...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>0.006584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>Vi er til grin! Totalt til grin. Stik ham en m...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.040093</td>\n",
       "      <td>0.040093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>Det er øverst fra Kronborg hvis nogen skulle v...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.001530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Jamen går rødekors penge ikke til dem i forvejen</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Direktøren på Huffington Post er en dansker!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.004052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>Har han kommenteret i dag?</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet subtask_a  \\\n",
       "id                                                                  \n",
       "924   Jeg ville ønske jeg kunne anerkende [KORRUPT P...       OFF   \n",
       "159                Fuck perkerne. ses på 4chan, tabere.       OFF   \n",
       "658                         Fuckkkkk det her er mig....       OFF   \n",
       "9     Hvordan i helvede fik de overhovedet dit numme...       OFF   \n",
       "137                            @USER ryger du hash. ???       OFF   \n",
       "1139  Var vi de eneste røvhuller som dyppede karamel...       OFF   \n",
       "544   Normalt synes jeg Marx var lige højreorientere...       OFF   \n",
       "3545  Potentiale til månedens billede, lige der.  De...       OFF   \n",
       "3326                Det fandme en fugtig migmig det der       OFF   \n",
       "753   uuh, denne her bliver nok upopulær, men jeg er...       OFF   \n",
       "777   Jeg mener ikke at en mand er mere troværdig. J...       NOT   \n",
       "621   Tillykke til både Danmark og Island  - Hilsen ...       NOT   \n",
       "1089  Selv mod Trump kæmper guderne forgæves han er ...       NOT   \n",
       "927                Håber der er Fodbold på den tid i TV       NOT   \n",
       "379   Hun er jo ikke dum. Jeg respektere da helt kla...       NOT   \n",
       "1487  Vi er til grin! Totalt til grin. Stik ham en m...       NOT   \n",
       "2532  Det er øverst fra Kronborg hvis nogen skulle v...       NOT   \n",
       "108    Jamen går rødekors penge ikke til dem i forvejen       NOT   \n",
       "860        Direktøren på Huffington Post er en dansker!       NOT   \n",
       "3122                         Har han kommenteret i dag?       NOT   \n",
       "\n",
       "      p_offensive_no_aug  p_offensive_upper  \n",
       "id                                           \n",
       "924             0.993760           0.993760  \n",
       "159             0.995237           0.995237  \n",
       "658             0.013337           0.013337  \n",
       "9               0.972994           0.972994  \n",
       "137             0.997740           0.997740  \n",
       "1139            0.926999           0.926999  \n",
       "544             0.125608           0.125608  \n",
       "3545            0.001631           0.001631  \n",
       "3326            0.022390           0.022390  \n",
       "753             0.021918           0.021918  \n",
       "777             0.000148           0.000148  \n",
       "621             0.000064           0.000064  \n",
       "1089            0.003527           0.003527  \n",
       "927             0.000501           0.000501  \n",
       "379             0.006584           0.006584  \n",
       "1487            0.040093           0.040093  \n",
       "2532            0.001530           0.001530  \n",
       "108             0.000141           0.000141  \n",
       "860             0.004052           0.004052  \n",
       "3122            0.000324           0.000324  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be a bit more explicit we can also compare it using summary information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The augmentation lead to classification changes in 0/20\n",
      "The average prob. of OFF went from 0.507(0.497) to 0.507(0.497).\n",
      "The average prob. of NOT went from 0.006(0.012) to 0.006(0.012).\n"
     ]
    }
   ],
   "source": [
    "def compare_cols(\n",
    "    augmentation,\n",
    "    baseline=mini_test[\"p_offensive_no_aug\"],\n",
    "    category=mini_test[\"subtask_a\"],\n",
    "):\n",
    "    \"\"\"Compares augmentation with the baseline for each of the categories\"\"\"\n",
    "    changes = ((augmentation > 0.5) != (baseline > 0.5)).sum()\n",
    "    n = len(augmentation)\n",
    "    print(f\"The augmentation lead to classification changes in {changes}/{n}\")\n",
    "    for cat in set(category):\n",
    "        aug_cat_mean = augmentation[category == cat].mean().round(3)\n",
    "        aug_cat_std = augmentation[category == cat].std().round(3)\n",
    "        cat_mean = baseline[category == cat].mean().round(3)\n",
    "        cat_std = baseline[category == cat].std().round(3)\n",
    "        print(\n",
    "            f\"The average prob. of {cat} went from {cat_mean}({cat_std}) to {aug_cat_mean}({aug_cat_std}).\"\n",
    "        )\n",
    "\n",
    "compare_cols(mini_test[\"p_offensive_upper\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises:\n",
    "\n",
    "1) Solve the above mystery, why doesn't the model estimate change might when uppercasing? *Hint*: Check the tokenizer of the model\n",
    "2) Examining the data, I seemed to notice that spelling error were more common among offensive tweets. Is this correct? [*Hint*](https://kennethenevoldsen.github.io/augmenty/augmenty.character.html?highlight=keystroke#augmenty.character.replace.create_keystroke_error_augmenter)\n",
    "3) Examine the data yourself and create three hypothesis on what augmentation might change the performance.\n",
    "4) Outline how you could apply augmentation (behavioral testing) to examine a model (or pipeline) in your project\n",
    "5) (Optional): Apply this behavioural testing to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Probably because the tokenizer lower-cases everything (also when you made it upper-case)\n",
    "# yes, it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweet subtask_a  \\\n",
      "id                                                                  \n",
      "924   Jeg ville ønske jeg kunne anerkende [KORRUPT P...       OFF   \n",
      "159                Fuck perkerne. ses på 4chan, tabere.       OFF   \n",
      "658                         Fuckkkkk det her er mig....       OFF   \n",
      "9     Hvordan i helvede fik de overhovedet dit numme...       OFF   \n",
      "137                            @USER ryger du hash. ???       OFF   \n",
      "1139  Var vi de eneste røvhuller som dyppede karamel...       OFF   \n",
      "544   Normalt synes jeg Marx var lige højreorientere...       OFF   \n",
      "3545  Potentiale til månedens billede, lige der.  De...       OFF   \n",
      "3326                Det fandme en fugtig migmig det der       OFF   \n",
      "753   uuh, denne her bliver nok upopulær, men jeg er...       OFF   \n",
      "777   Jeg mener ikke at en mand er mere troværdig. J...       NOT   \n",
      "621   Tillykke til både Danmark og Island  - Hilsen ...       NOT   \n",
      "1089  Selv mod Trump kæmper guderne forgæves han er ...       NOT   \n",
      "927                Håber der er Fodbold på den tid i TV       NOT   \n",
      "379   Hun er jo ikke dum. Jeg respektere da helt kla...       NOT   \n",
      "1487  Vi er til grin! Totalt til grin. Stik ham en m...       NOT   \n",
      "2532  Det er øverst fra Kronborg hvis nogen skulle v...       NOT   \n",
      "108    Jamen går rødekors penge ikke til dem i forvejen       NOT   \n",
      "860        Direktøren på Huffington Post er en dansker!       NOT   \n",
      "3122                         Har han kommenteret i dag?       NOT   \n",
      "\n",
      "      p_offensive_no_aug  p_offensive_upper  p_keystroke_error  \\\n",
      "id                                                               \n",
      "924             0.993760           0.993760           0.006027   \n",
      "159             0.995237           0.995237           0.002658   \n",
      "658             0.013337           0.013337           0.004347   \n",
      "9               0.972994           0.972994           0.003511   \n",
      "137             0.997740           0.997740           0.008164   \n",
      "1139            0.926999           0.926999           0.004592   \n",
      "544             0.125608           0.125608           0.007933   \n",
      "3545            0.001631           0.001631           0.012909   \n",
      "3326            0.022390           0.022390           0.000745   \n",
      "753             0.021918           0.021918           0.004804   \n",
      "777             0.000148           0.000148           0.006035   \n",
      "621             0.000064           0.000064           0.003781   \n",
      "1089            0.003527           0.003527           0.002168   \n",
      "927             0.000501           0.000501           0.002190   \n",
      "379             0.006584           0.006584           0.004260   \n",
      "1487            0.040093           0.040093           0.004195   \n",
      "2532            0.001530           0.001530           0.005487   \n",
      "108             0.000141           0.000141           0.003547   \n",
      "860             0.004052           0.004052           0.002740   \n",
      "3122            0.000324           0.000324           0.001635   \n",
      "\n",
      "      p_offensive_keystroke_error  \n",
      "id                                 \n",
      "924                      0.999302  \n",
      "159                      0.993182  \n",
      "658                      0.773904  \n",
      "9                        0.989061  \n",
      "137                      0.997467  \n",
      "1139                     0.963933  \n",
      "544                      0.995904  \n",
      "3545                     0.000121  \n",
      "3326                     0.011757  \n",
      "753                      0.857409  \n",
      "777                      0.001066  \n",
      "621                      0.000064  \n",
      "1089                     0.001048  \n",
      "927                      0.000501  \n",
      "379                      0.009298  \n",
      "1487                     0.031982  \n",
      "2532                     0.002974  \n",
      "108                      0.000082  \n",
      "860                      0.000701  \n",
      "3122                     0.000324  \n",
      "The augmentation lead to classification changes in 3/20\n",
      "The average prob. of OFF went from 0.507(0.497) to 0.758(0.403).\n",
      "The average prob. of NOT went from 0.006(0.012) to 0.005(0.01).\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "# load an augmenter\n",
    "keystroke_augmenter = augmenty.load(\"keystroke_error.v1\", level=0.05) # augment 5%\n",
    "\n",
    "# Apply\n",
    "aug_texts = augmenty.texts(texts, augmenter=keystroke_augmenter, nlp=nlp)\n",
    "mini_test[\"p_offensive_keystroke_error\"] = apply(list(aug_texts))\n",
    "\n",
    "# Comparing\n",
    "print(compare_cols(mini_test[\"p_offensive_keystroke_error\"]))\n",
    "\n",
    "# No clear trend when doing it on only 20 samples. The model is not robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>p_offensive_no_aug</th>\n",
       "      <th>p_offensive_upper</th>\n",
       "      <th>p_keystroke_error</th>\n",
       "      <th>p_offensive_keystroke_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Jeg ville ønske jeg kunne anerkende [KORRUPT P...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.993760</td>\n",
       "      <td>0.993760</td>\n",
       "      <td>0.006027</td>\n",
       "      <td>0.999302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Fuck perkerne. ses på 4chan, tabere.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.995237</td>\n",
       "      <td>0.995237</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.993182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>Fuckkkkk det her er mig....</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.013337</td>\n",
       "      <td>0.013337</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.773904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hvordan i helvede fik de overhovedet dit numme...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.972994</td>\n",
       "      <td>0.972994</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.989061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>@USER ryger du hash. ???</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.997740</td>\n",
       "      <td>0.997740</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>0.997467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>Var vi de eneste røvhuller som dyppede karamel...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.926999</td>\n",
       "      <td>0.926999</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.963933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Normalt synes jeg Marx var lige højreorientere...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.125608</td>\n",
       "      <td>0.125608</td>\n",
       "      <td>0.007933</td>\n",
       "      <td>0.995904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545</th>\n",
       "      <td>Potentiale til månedens billede, lige der.  De...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.012909</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>Det fandme en fugtig migmig det der</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.022390</td>\n",
       "      <td>0.022390</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.011757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>uuh, denne her bliver nok upopulær, men jeg er...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.857409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Jeg mener ikke at en mand er mere troværdig. J...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.001066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Tillykke til både Danmark og Island  - Hilsen ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>Selv mod Trump kæmper guderne forgæves han er ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.001048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>Håber der er Fodbold på den tid i TV</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Hun er jo ikke dum. Jeg respektere da helt kla...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>0.009298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>Vi er til grin! Totalt til grin. Stik ham en m...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.040093</td>\n",
       "      <td>0.040093</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>0.031982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>Det er øverst fra Kronborg hvis nogen skulle v...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.005487</td>\n",
       "      <td>0.002974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Jamen går rødekors penge ikke til dem i forvejen</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Direktøren på Huffington Post er en dansker!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.000701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>Har han kommenteret i dag?</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet subtask_a  \\\n",
       "id                                                                  \n",
       "924   Jeg ville ønske jeg kunne anerkende [KORRUPT P...       OFF   \n",
       "159                Fuck perkerne. ses på 4chan, tabere.       OFF   \n",
       "658                         Fuckkkkk det her er mig....       OFF   \n",
       "9     Hvordan i helvede fik de overhovedet dit numme...       OFF   \n",
       "137                            @USER ryger du hash. ???       OFF   \n",
       "1139  Var vi de eneste røvhuller som dyppede karamel...       OFF   \n",
       "544   Normalt synes jeg Marx var lige højreorientere...       OFF   \n",
       "3545  Potentiale til månedens billede, lige der.  De...       OFF   \n",
       "3326                Det fandme en fugtig migmig det der       OFF   \n",
       "753   uuh, denne her bliver nok upopulær, men jeg er...       OFF   \n",
       "777   Jeg mener ikke at en mand er mere troværdig. J...       NOT   \n",
       "621   Tillykke til både Danmark og Island  - Hilsen ...       NOT   \n",
       "1089  Selv mod Trump kæmper guderne forgæves han er ...       NOT   \n",
       "927                Håber der er Fodbold på den tid i TV       NOT   \n",
       "379   Hun er jo ikke dum. Jeg respektere da helt kla...       NOT   \n",
       "1487  Vi er til grin! Totalt til grin. Stik ham en m...       NOT   \n",
       "2532  Det er øverst fra Kronborg hvis nogen skulle v...       NOT   \n",
       "108    Jamen går rødekors penge ikke til dem i forvejen       NOT   \n",
       "860        Direktøren på Huffington Post er en dansker!       NOT   \n",
       "3122                         Har han kommenteret i dag?       NOT   \n",
       "\n",
       "      p_offensive_no_aug  p_offensive_upper  p_keystroke_error  \\\n",
       "id                                                               \n",
       "924             0.993760           0.993760           0.006027   \n",
       "159             0.995237           0.995237           0.002658   \n",
       "658             0.013337           0.013337           0.004347   \n",
       "9               0.972994           0.972994           0.003511   \n",
       "137             0.997740           0.997740           0.008164   \n",
       "1139            0.926999           0.926999           0.004592   \n",
       "544             0.125608           0.125608           0.007933   \n",
       "3545            0.001631           0.001631           0.012909   \n",
       "3326            0.022390           0.022390           0.000745   \n",
       "753             0.021918           0.021918           0.004804   \n",
       "777             0.000148           0.000148           0.006035   \n",
       "621             0.000064           0.000064           0.003781   \n",
       "1089            0.003527           0.003527           0.002168   \n",
       "927             0.000501           0.000501           0.002190   \n",
       "379             0.006584           0.006584           0.004260   \n",
       "1487            0.040093           0.040093           0.004195   \n",
       "2532            0.001530           0.001530           0.005487   \n",
       "108             0.000141           0.000141           0.003547   \n",
       "860             0.004052           0.004052           0.002740   \n",
       "3122            0.000324           0.000324           0.001635   \n",
       "\n",
       "      p_offensive_keystroke_error  \n",
       "id                                 \n",
       "924                      0.999302  \n",
       "159                      0.993182  \n",
       "658                      0.773904  \n",
       "9                        0.989061  \n",
       "137                      0.997467  \n",
       "1139                     0.963933  \n",
       "544                      0.995904  \n",
       "3545                     0.000121  \n",
       "3326                     0.011757  \n",
       "753                      0.857409  \n",
       "777                      0.001066  \n",
       "621                      0.000064  \n",
       "1089                     0.001048  \n",
       "927                      0.000501  \n",
       "379                      0.009298  \n",
       "1487                     0.031982  \n",
       "2532                     0.002974  \n",
       "108                      0.000082  \n",
       "860                      0.000701  \n",
       "3122                     0.000324  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "#Examine the data yourself and create three hypothesis on what augmentation might change the performance.\n",
    "#Kenneth looked at @user (maybe more likely to be offensive, when you are directing the tweet towards someone)\n",
    "# and looked at !!! use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "#Outline how you could apply augmentation (behavioral testing) to examine a model (or pipeline) in your project"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "021482b7625aaacc2d343324781c6ce2f121934a239bde69eda2b56fdffea080"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('NLP': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
