{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Calculate the dot product between two word embeddings which you believe are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as nn\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.41781"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Two wordembeddings\n",
    "apple_vec = model[\"apple\"]\n",
    "banana_vec = model[\"banana\"]\n",
    "\n",
    "#dot product (=length of the projected vector)\n",
    "np.dot(apple_vec, banana_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2) Calculate the dot product between the two word and a word which you believe is dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6469955\n",
      "6.790193\n"
     ]
    }
   ],
   "source": [
    "war_vec = model[\"war\"]\n",
    "\n",
    "#dot\n",
    "print(np.dot(apple_vec, war_vec)) #lower\n",
    "print(np.dot(banana_vec, war_vec)) #lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 3) make the three words into a matrix $E$ and multiply it with its own transpose using matrix multiplication. So $E \\cdot E^T$\n",
    "  - what does the values in matric correspond to? What do you imagine the dot product is? *Hint*, similarity between vectors (cosine similarity) is exactly the same as the dot product assuming you normalize the lenghth of each vector to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 50)\n",
      "[[28.009117 14.417807  5.646996]\n",
      " [14.417807 23.599033  6.790193]\n",
      " [ 5.646996  6.790193 35.105724]]\n"
     ]
    }
   ],
   "source": [
    "E = np.asmatrix([apple_vec, banana_vec, war_vec])\n",
    "print(E.shape)\n",
    "\n",
    "#transpose\n",
    "E_t = E.T\n",
    "\n",
    "#matrix multipication @\n",
    "v = E @ E.T\n",
    "print(v)\n",
    "\n",
    "# Matrix values in the diagonal (top/left to bottom/right) is the dot product between itself and itself -\n",
    "# that's why they are so similar.\n",
    "# corresponds to the dot product between all combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 4) Examine the attention formula from Vaswani et al. (2017), you have now implemented $Q\\cdot K^T$\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d}}) \\cdot V\n",
    "$$\n",
    "Where $d$ is the dimension of of the embedding and Q, K, V stands for queries, keys and values.\n",
    "\n",
    "\n",
    "  - 4.1) Now add the softmax. Examining the outcome, how come that the matrix is no longer symmetric?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 1.2513e-06, 1.9420e-10],\n",
      "        [1.0294e-04, 9.9990e-01, 5.0115e-08],\n",
      "        [1.6078e-13, 5.0434e-13, 1.0000e+00]])\n",
      "[[9.9999881e-01 1.2513199e-06 1.9420189e-10]\n",
      " [1.0294356e-04 9.9989700e-01 5.0115133e-08]\n",
      " [1.6078171e-13 5.0433746e-13 1.0000000e+00]]\n",
      "1.000000059421179\n",
      "0.9999999968478477\n",
      "1.000000000000665\n"
     ]
    }
   ],
   "source": [
    "#adding softmax\n",
    "v_ten = nn.tensor(v)\n",
    "soft = nn.softmax(v_ten, dim = 1)\n",
    "print(soft)\n",
    "\n",
    "#back to np\n",
    "soft = np.asarray(soft)\n",
    "print(soft)\n",
    "\n",
    "#not symmetric (cannot be mirrored in the diagonal)\n",
    "print(sum(soft[0]))\n",
    "print(sum(soft[1]))\n",
    "print(sum(soft[2]))\n",
    "#all rows add up to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 4.2) Now normalize the using the $\\sqrt{d}$, how does this change the outcome?\n",
    "\n",
    "1) the matrix resulting from the softmax is referred to as the attention matrix and is how much each matrix should pay attention to the others when we multiply our attention matrix by our matrix $E$ (corresponding to $V$). Try it out:\n",
    "\n",
    "- 5.1) This is essentially a weighted sum, one way to see this is to extract the weight from the first row of the matrix"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f20a2b0473630642730946317169bdea332e5675536504f1ca8796679a7e5286"
  },
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit ('nlp_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
